\section{Przyjęta metodyka}
Projekt rozpoczęliśmy od stworzenia prototypu strony serwerowej,
wykorzystując najprostszą reprezentację strukturalną dla posiadanych artykułów, czyli wektor słów dla każdego z nich.

Od tego momentu kolejne funkcjonalności dodawane były przyrostowo. Kolejne spotkania z klientem wyznaczały końce kolejnych iteracji. Tym sposobem ograniczone zostało ryzyko niedostarczenia projektu - posiadając pod koniec każdej iteracji działającą stronę serwerową groziło w najgorszym przypadku okrojeniem funkcjonalności, który to scenariusz na szczęscie nie miał miejsca.

\section{Podział pracy}
\subsection{Michał Adamczyk}
\begin{itemize}
  \item Czyszczenie artykułów - stop words removal, stemming
  \item Budowa reprezentacji strukturalnej artykułów w postaci wektorów cech
  \item Redukcja wymiaru macierzy wektorów cech - za pomocą algorytmu SVD lub algorytmu LDA (stosowane wymiennie)
  \item Dołączenie wiedzy zewnętrznej w postaci kategorii z bazy danych artykułów reutersa
  \item Implementacja algorytmu k-means++
  \item Etykietowanie klastrów najbardziej charakterystycznymi dla nich słowami oraz nazwami kategorii z bazy zewnętrznej
\end{itemize}
\subsection{Dominik Majda}
\begin{itemize}
  \item Budowa reprezentacji strukturalnej artykułów w postaci engramów oraz wyliczenie na ich podstawie grupy niezmienników grafowych
  \item Zdobycie za pomocą web crawlingu zbioru artykułów o tematyce finansowej
  \item Przekształcenie pozyskanych artykułów z formatu html do czystego tekstu. Odfiltrowanie nieistotnych z nich - na przykład zbyt krótkich.
  \item Stworzenie aplikacji mobilnej realizującej rolę interfejsu końcowego po stronie użytkownika
  \item Wystawienie interfejsu serwera w postaci restowego API dostępnego na przykład dla stworzonej przez nas aplikacji mobilnej 
 \item Budowa bazy danych dla artykułów z obu zbiorów dancych, dzięki czemu możliwe jest przechowywanie dodatkowych o artykułach informacji używanych w trakcie obliczeń (jak link czy tytuł).  
\end{itemize}